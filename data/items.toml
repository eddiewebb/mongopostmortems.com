[[items]]
title = "Epic Games - Fortnite"
image = "https://cdn2.unrealengine.com/Fortnite%2Fblog%2Fpostmortem-of-service-outage-4-12%2FBR04_News_Featured_Postmortem-of-Service-Outage-4-11-and-4-12-18_v2-1920x1080-2ffda468afaceac2fc7529fe847efc3a876e298d.jpg"
thumb = "thumbnails/fortnite.png"
alt = "Epic games - fortnite"
description = "At a high level, the Mongo database that backs our account service experienced severe performance degradation, which in turn caused our account service to be unavailable from April 11th 20:44 UTC to April 12th 14:30 UTC.  This means all login requests, account creations, and queries for account information failed. "
url = "https://www.epicgames.com/fortnite/en-US/news/postmortem-of-service-outage-4-12"

[[items]]
title = "Updown.io"
image = "thumbnails/updown.png"
thumb = "thumbnails/updown.png"
alt = "Updown.io"
description = "April 3rd at 2:30 am: updown.io primary database (MongoDB) server starts to answer queries about 200 times slower than usual, causing an almost total outage of the website and the monitoring daemons"
url = "https://medium.com/@adrienjarthon/updown-io-incident-postmortem-f4707cd20091"


[[items]]
title = "Discord"
image = "thumbnails/discord.png"
thumb = "thumbnails/discord.png"
alt = "Discord"
description = "We are continuing to see impact to one of our MongoDB machines which is causing the recovery to be slow and is what kicked off this problem. Google has confirmed this is a problem with their storage layer and they are escalating internally to try to achieve resolution."
url = "https://status.discordapp.com/incidents/cg7y5gbl80pg"

[[items]]
title = "Mailgun"
image = "thumbnails/mailgun.png"
thumb = "thumbnails/mailgun.png"
alt = "mailgun"
description = "On January 12th at 14:09 UTC, our engineers began responding to alerts for our click / open tracking services and primary MongoDB clusters located in one of our production regions. Within ten minutes, the engineering team had determined that our MongoDB secondary servers, which are used for most read operations, were under heavy load and failing to serve requests in a timely manner."
url = "https://status.mailgun.com/incidents/p9nxxql8g9rh"



[[items]]
title = "Foursquare"
image = "thumbnails/foursquare.jpg"
thumb = "thumbnails/foursquare.jpg"
alt = "Foursquare"
description = "Foursquare recently suffered a total site outage for eleven hours. The outage was caused by unexpected uneven growth in their MongoDB database that their monitoring didn't detect. The system outage was prolonged when an attempt to add a partition didn't work due to fragmentation, and required taking the database offline to compact it. "
url = "https://www.infoq.com/news/2010/10/4square_mongodb_outage"


[[items]]
title = "About You"
image = "thumbnails/aboutyou.png"
thumb = "thumbnails/aboutyou.png"
alt = "About You"
description = "The first real indication that the MongoDB was not a happy bunny came a couple of hours later at 10pm, when the sending of transactional mails slowed down to a crawl (our queue length alerts were doing overtime)."
url = "https://medium.com/about-developer-blog/the-day-our-mongodb-went-down-bbbc9db1af66"


[[items]]
title = "logDNA"
image = "thumbnails/logdna.png"
thumb = "thumbnails/logdna.png"
alt = "logDNA"
description = "Our database (MongoDB) died approximately 5:19am UTC. The primary was stuck in a connection loop (out of threads for new connections). We got the alert, but it seemed to have self-healed thru migrating to its secondary. The primary resynced and everything appeared to be fine. The alerts turned back green. What we didn't realize was that between 5:44am and 8:20am UTC, there was a series of small hiccups (queue buildups, flushes, repeat) that we hadn't noticed. Rollbar complained about Mongo connection errors sporadically."
url = "https://status.logdna.com/incidents/194mdmw4qjm9"


[[items]]
title = "Close"
image = "thumbnails/close.png"
thumb = "thumbnails/close.png"
alt = "Close"
description = "Over the weekend, on Saturday September 19th, we upgraded some of our database servers from MongoDB 2.6.11 to MongoDB 3.0.6. We did this following recommendations from MongoDB's official release notes, documentation, and the consultant we engaged with, with a goal of leading to better performance and stability. Unfortunately, it failed at that quite spectacularly and we did a bad job stress testing the upgraded version before sending production traffic to it."
url = "https://status.close.com/incidents/t43lb0yf5h4z"


[[items]]
title = "anynines"
image = "thumbnails/anynines.png"
thumb = "thumbnails/anynines.png"
alt = "anynines"
description = "We're investigating some surround systems and MongoDB services that are currently still not available"
url = "https://status.anynines.com/incidents/p0s8sjd0j11z"



[[items]]
title = "Universe"
image = "thumbnails/universe.png"
thumb = "thumbnails/universe.png"
alt = "Universe"
description = "Universe believes that the issue began at 12:02 GMT, when we began to process webhooks from Stripe. In reviewing our Mongo operation log, we notice ~2300 of queries generated by this processing activity. Under load, some of these queries individually consumed more than 10 minutes of processing time, eating a Read Ticket by Mongoâ€™s storage engine WiredTiger. Our configuration provides us with 128 Read Tickets, and so just 128 of these events processing simultaneously consumes all of them, generating large Reader Queues. During this time, CPU utilization on the primary node hit, and remained at 100%. At 12:08 GMT when we brought additional application-tier capacity online, our new app servers created additional connections at a time when the primary node had 0 Read Tickets available, 100% CPU utilization, and a Reader Queue of over 1k. Immediately after these connections, we observe the unresponsiveness of the system via a number of external pingdoms."
url = "http://status.universe.com/incidents/qms7hd5plgmq"

[[items]]
title = "Contactually"
image = "thumbnails/contactually.png"
thumb = "thumbnails/contactually.png"
alt = "Contactually"
description = "We've identified some fail points in our application related to our MongoDB setup and have already started taking steps to address them. We are changing our DB and host solution to one that will provide much more room for growth and greater stability. We should be better suited to handle an unexpected issue should one arise in the future."
url = "https://status.contactually.com/incidents/6xwxjxw237m1"

[[items]]
title = "Ping Identity"
image = "thumbnails/pingidentity.png"
thumb = "thumbnails/pingidentity.png"
alt = "Ping Identity"
description = "Monitoring systems have detected an issue with SCIM Provisioning Service. The Site Reliability Engineering team has been notified and is currently working the issue. We will update this message when the incident has been identified. Automated monitoring systems will update affected components and will resolve operational status as systems recover. "
url = "https://status.pingidentity.com/incidents/1k0ybqdt4kyj"


[[items]]
title = "Dispatch"
image = "thumbnails/dispatch.png"
thumb = "thumbnails/dispatch.png"
alt = "Dispatch"
description = "We were able to detect this issue due to an alert that notified us about issues with url-shortener. In regards to remediation and prevention, we have new initiatives to change our url-shortener service so that it no longer relies on MongoDB at this particular provider. We will either plan to host our own MongoDB or find a solution that moves us away from the dependency on this provider. Another plan for prevention is to implement some sort of immediacy logic for specific events in order to prevent notifications such as \"On My Way\" to be sent out if the window of time is no longer relevant."
url = "https://status.dispatch.me/incidents/3br52gpbrzfp"



#[[items]]
#title = "Hugrid Theme"
#image = "thumbnails/anynines.png"
#thumb = "thumbnails/anynines.png"
#alt = "Hugrid Theme"
#description = ""
#url = "https://github.com/aerohub/hugrid"
